{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ffe84f",
   "metadata": {},
   "source": [
    "# 5.1 계산그래프\n",
    "그래프:여러개의 노드와 에지로 표현되는 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace69414",
   "metadata": {},
   "source": [
    "## 5.1.1 계산그래프로 풀다  \n",
    "순전파:계산을 왼쪽에서 오른쪽으로 진행하는 단계  \n",
    "역전파:순전파와 반대방향  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7eceb7",
   "metadata": {},
   "source": [
    "## 5.1.2 국소적 계산  \n",
    "cf)국소적:자신과 직접 관계된 작은 범위  \n",
    "여기서 각 노드는 결국 자신과 관련된 계산(ex)입력한 두숫자의 덧셈)외에는 신경 쓸 것이 없다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbcfe3",
   "metadata": {},
   "source": [
    "## 5.1.3 왜 계산그래프로 푸는가?  \n",
    "### 계산그래프의 이점:\n",
    "- 국소적 계산  \n",
    "- 중간계산결과를 모두 보관가능  \n",
    "- 역전파를 통해 미분을 효율적으로 계산 가능  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fbd7d6",
   "metadata": {},
   "source": [
    "# 5.2 연쇄법칙\n",
    "연쇄법칙:국소적 미분을 전달할때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8b403",
   "metadata": {},
   "source": [
    "## 5.2.1 계산 그래프의 역전파  \n",
    "역전파의 계산 절차는 신호(E)에 노드의 국소적미분을 곱한 후 다음 노드로 전달하는 것  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113c993",
   "metadata": {},
   "source": [
    "## 5.2.2 연쇄법칙이란?  \n",
    "합성함수:여러함수로 구성된 함수  \n",
    "ex)z=t**2 이때 t=x+y  \n",
    "합성함수의 미분은 합성함수를 구성하는 각 함수의 미분 곱으로 나타낼 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbffd02",
   "metadata": {},
   "source": [
    "## 5.2.3 연쇄법칙과 계산 그래프  \n",
    "역전차가 하는 일은 연쇄법칙의 원리와 같다고 할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665aed47",
   "metadata": {},
   "source": [
    "# 5.3 역전파  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f4674",
   "metadata": {},
   "source": [
    "## 5.3.1 덧셈노드의 역전파  \n",
    "z=x+y라는 식을 대상으로 그 역전파를 살펴본다고 가정.x에 대한 z의 미분은 1,y에 대한 z의 미분도 1이다.  \n",
    "상류로부터 z에 대한 L의 미분값이 전해지고 다시 하류로 x에 대한 L의 미분,y에 대한 L의 미분값을 전달한다.  \n",
    "if)덧셈노드 가정시(10+5=15이고 상류값이 1.3), 덧셈노드 역전파는 입력신호를 다음 노드로 출력할 뿐이므로 1.3을 그대로 다음 노드에 전달한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cb397",
   "metadata": {},
   "source": [
    "## 5.3.2 곱셈노드의 역전파  \n",
    "if)z=xy이면, x에 대한 z의 미분은 y,y에 대한 z의 미분은 x이다.  \n",
    "곱셈노드 역전파는 상류값에 순전파때의 입력신호들을 서로 바꾼 값을 곱해서 하류로 보낸다  \n",
    "곱셈의 역전파는 순방향 입력신호의 값이 필요하다.그래서 곱셈 노드 구현시, 순전파의 입력신호를 변수에 저장  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c626fac",
   "metadata": {},
   "source": [
    "# 5.4 단순한 계층 구현하기  \n",
    "가정)계산그래프의 곱셈노드를 mulLayer,덧셈노드를 addLayer라는 이름 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291edd6",
   "metadata": {},
   "source": [
    "## 5.4.1곱셈계층  \n",
    "forward():순전파  \n",
    "backward():역전파  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a91aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):#변수x와 y초기화(순전파의 입력값 유지하는 목적으로 사용)\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "    def forward(self,x,y):#x,y를 인수로 받고 두 값을 곱해서 반환\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        out=x*y\n",
    "        return out\n",
    "    def backward(self,dout):#상류에 넘어온 미분에 순전파 때의 값을 서로 바꿔 곱한 후 하류로 흘림\n",
    "        dx=dout*self.y#x와 y를 바꾼다  \n",
    "        dy=dout*self.x\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba765d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple=100\n",
    "apple_num=2\n",
    "tax=1.1\n",
    "\n",
    "#계층들\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price=mul_apple_layer.forward(apple,apple_num)\n",
    "price=mul_tax_layer.forward(apple_price,tax)\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910aaee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "#각 변수에 대한 미분-역전파\n",
    "dprice=1\n",
    "dapple_price,dtax=mul_tax_layer.backward(dprice)\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "print(dapple,dapple_num,dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aab92f",
   "metadata": {},
   "source": [
    "cf)주의:backward가 받는 인수는 \"순전파의 출력에 대한 미분\" 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce0ed4",
   "metadata": {},
   "source": [
    "## 5.4.2 덧셈계층  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9738dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        out=x+y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208d98af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple=100\n",
    "apple_num=2\n",
    "orange=150\n",
    "orange_num=3\n",
    "tax=1.1\n",
    "\n",
    "#계층들\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_orange_layer=MulLayer()\n",
    "add_apple_orange_layer=AddLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price=mul_apple_layer.forward(apple,apple_num)\n",
    "orange_price=mul_orange_layer.forward(orange,orange_num)\n",
    "all_price=add_apple_orange_layer.forward(apple_price,orange_price)\n",
    "price=mul_tax_layer.forward(all_price,tax)\n",
    "\n",
    "#역전파\n",
    "dprice=1\n",
    "dall_price,dtax=mul_tax_layer.backward(dprice)\n",
    "dapple_price,dorange_price=add_apple_orange_layer.backward(dall_price)\n",
    "dorange,dorange_num=mul_orange_layer.backward(dorange_price)\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num,dapple,dorange,dorange_num,dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea71b1a",
   "metadata": {},
   "source": [
    "# 5.5활성화 함수 계층 구현하기  \n",
    "신경망을 구성하는 층(계층)을 클래스 하나씩 구현한다.(ReLu & Sigmoid계층 구현)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003230d",
   "metadata": {},
   "source": [
    "ReLU: 순전파때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 보낸다.  \n",
    "순전파때가 x가 0이하면 역전파때는 하류로 신호를 보내지 않는다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7381f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "        \n",
    "    def forward(self,x):#순전파\n",
    "        self.mask=(x<=0)#x가 0이하면 self.mask을 out에 복사해서 0값으로 만들고 아니면 그냥 복사하기 입력값 그대로!\n",
    "        out=x.copy()\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):#역전파\n",
    "        dout[self.mask]=0#상류의 값을 그대로 하류에 전달\n",
    "        dx=dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5598259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([[1.0,-0.5],[-2.0,3.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e518eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask=(x<=0)\n",
    "print(mask)#순전파때 입력이 0이하면 역전파값은 0->역전파때는 순전파때의 mask를 써서 True인 곳은 dout을 0으로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad81263",
   "metadata": {},
   "source": [
    "## 5.5.2 시그모이드 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f297a",
   "metadata": {},
   "source": [
    "1단계 '/' 노드, y = 1 / x를 미분하면 다음식이 됨\n",
    "역전파 때는 상류의 예측값에 -y**2 을 곱해서 하류로 전달\n",
    "\n",
    "2단계 상류의 값을 여과 없이 하류로 보냄\n",
    "\n",
    "3단계 y = exp(x) 연산을 수행\n",
    "계산 그래프에서는 상류의 순전파 때의 출력(exp(-x))을 곱해 하류로 전파\n",
    "\n",
    "4단계 y = exp(x) 연산을 수행\n",
    "'X' 노드, 순전파 때의 값을 서로 바꿔 곱함. 이 경우,-1을 곱함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949986f",
   "metadata": {},
   "source": [
    "시그모이드 간소화버전  \n",
    "노드를 그룹화하여 Sigmoid 계층의 세세한 내용을 노출하지 않고 입력과 출력에만 집중  \n",
    "Sigmoid 계층의 계산 그래프: 순전파의 출력 y만으로 역전파를 계산  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97bfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out=None\n",
    "        \n",
    "    def forward(self,x):#순전파\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):#역전파\n",
    "        dx=dout*(1.0-self.out)*self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d3caf",
   "metadata": {},
   "source": [
    "# 5.6 Affine/Softmax계층 구현하기  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5538c1",
   "metadata": {},
   "source": [
    "## 5.6.1 Affine계층  \n",
    "affine계층:어파인 변화을 수행하는 처리  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b2ab5",
   "metadata": {},
   "source": [
    "주의사항: 어핀계층의 계산 그래프에서 행렬 곱에 주의를 기울인다.  \n",
    "- 행렬 곱에서 대응하는 차원의 원소수를 일치시켜야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8245b16",
   "metadata": {},
   "source": [
    "## 5.6.2 배치용 Affine계층  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86301f43",
   "metadata": {},
   "source": [
    "입력 X의 형상(shape)이 (N,2)가 됨.  \n",
    "이후에는 순전파할때는 위와 동일, 역전파때는 행렬의 shape형상에 주의하면서 값 도출  \n",
    "if)N=2(데이터2개)인 경우, 편향은 두 데이터 각각에 더해진다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507abcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W=np.array([[0,0,0],[10,10,10]])\n",
    "B=np.array([1,2,3])\n",
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38532bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W+B#순전파의 편향 덧셈은 각각의 데이터 원소에 더해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68253ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY=np.array([[1,2,3],[4,5,6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb5b87",
   "metadata": {},
   "source": [
    "편향의 역전파는 두 데이터에 대한 미분을 데이터마다 더해서 구한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e0cf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB=np.sum(dY,axis=0)#np.sum()에서 0번재 축(column)에 대해 총합을 구한다.\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6fca662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        out=np.dot(x,self.W)+self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx=np.dot(dout,self.W.T)\n",
    "        self.dW=np.dot(self.x.T,dout)\n",
    "        self.db=np.sum(dout,axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f02978",
   "metadata": {},
   "source": [
    "## 5.6.3 Softmax-with-Loss계층  \n",
    "소프트맥스함수는 입력값을 정규화해서 출력한다  \n",
    "ex)Mnist데이터셋에서 손글씨 숫자는 가짓수가 10개(클래스 분류)이므로 softmax계층의 입력이 10개가 된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf2613",
   "metadata": {},
   "source": [
    "cf) 신경망에서 수행하는 작업은 '학습' 과 '추론' 이다.  \n",
    "추론시에는 소프트맥스함수를 사용하지 않고 신경망 추론시에는 마지막 Affine계층의 출력을 인식한다.(학습시에는 사용)  \n",
    "신경망에서 정규화되지 않는 출력결과에서는 softmax앞의 Affine계층의 출력을 점수(score)라고 한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213b947",
   "metadata": {},
   "source": [
    "소프트맥스 계층 구현: 손실 함수인 교차 엔트로피 오차도 포함하여 'Softmax-with-Loss 계층'이라는 이름으로 구현  \n",
    "Softmax 계층: 입력 (a1, a2, a3)를 정규화 및 (y1, y2, y3)를 출력  \n",
    "Cross Entropy 계층: Softmax의 출력(y1, y2, y3)과 정답 레이블(t1, t2, t3)를 받고, 손실 L을 출력  \n",
    "Softmax 계층의 역전파:(y1-t1, y2-t2, y3-t3)->softmax계층의 출력과 정답레이블의 차분  \n",
    "\n",
    "신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해짐  \n",
    "소프트맥스 함수의 손실 함수로 교차 엔트로피 오차를 사용하니 역전파가 (y1-t1, y2-t2, y3-t3)로 말끔히 떨어짐  \n",
    "- 결국 y1-t1,y2-t2...등은 softmax계층의 출력과 정답레이블의 차이로 \"신경망의 현재 출력\" 과 \"정답레이블의 오차\"를 나타냄  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c25af63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss=None#손실\n",
    "        self.y=None#softmax출력\n",
    "        self.t=None#정답레이블(원핫벡터)\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t=t\n",
    "        self.y=softmax(x)\n",
    "        self.loss=cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size=self.t.shape[0]\n",
    "        dx=(self.y-self.t)/batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5cff56",
   "metadata": {},
   "source": [
    "# 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a14ac1",
   "metadata": {},
   "source": [
    "## 5.7.1 신경망 학습의 전체 그림  \n",
    "전제  \n",
    "- 신경망에는 적응가능한 가중치, 편향이 존재  \n",
    "- 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정='학습'  \n",
    "\n",
    "1단계-미니배치  \n",
    "- 훈련데이터 일부를 무작위(미니배치)로 가져온다.  \n",
    "- 미니배치의 손실함수 값을 줄이는게 목표  \n",
    "\n",
    "2단계-기울기 산출  \n",
    "- 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기 구함  \n",
    "- (기울기:손실함수의 값을 줄여주는 방향 제시)  \n",
    "\n",
    "3단계-매개변수 갱신  \n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신  \n",
    "\n",
    "4단계-반복  \n",
    "- 1~3단계 반복  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1db5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from common.layers import *\n",
    "#from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 방지\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()      \n",
    "    return grad\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()                                           \n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) \n",
    "        self.layers['Relu1'] = Relu()                                         \n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2']) \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():                              \n",
    "            x = layer.forward(x)                                           \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):#가중치 매개변수의 기울기를 오차역전법으로 구하기!\n",
    "        # forward\n",
    "        self.loss(x, t)                  \n",
    "\n",
    "        # backward\n",
    "        dout = 1                             \n",
    "        dout = self.lastLayer.backward(dout) \n",
    "        \n",
    "        layers = list(self.layers.values())  \n",
    "        layers.reverse()                     \n",
    "        for layer in layers:                 \n",
    "            dout = layer.backward(dout)      \n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f85a3f1",
   "metadata": {},
   "source": [
    "OrderedDict: 딕셔너리에 추가한 순서를 기억하는 (순서가 있는) 딕셔너리  \n",
    "순전파 때는 추가한 순서대로 각 계층의 forward() 메서드를 호출  \n",
    "역전파 때는 계층을 반대 순서로 호출  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384bac3",
   "metadata": {},
   "source": [
    "## 5.7.3 오차역전파법으로 구한 기울기 검증하기  \n",
    "1. 수치미분으로 써서 구하는 방법  \n",
    "2. 수식을 해석적으로 풀어서 구하는 방법  \n",
    "수치미분법은 느리니까 오차역전파법을 이용하는 것이 더 좋음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c81a2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "591e8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.96 s ± 60.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit network.numerical_gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92e70915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 µs ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit network.gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7678bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.199807833724237e-13\n",
      "b1:8.407382158081427e-13\n",
      "W2:9.163717933069732e-13\n",
      "b2:1.2057021631095565e-10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84b770",
   "metadata": {},
   "source": [
    "결론: 실제로 numerical_gradient는 gradient보다 느리다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606ea88",
   "metadata": {},
   "source": [
    "## 5.7.4 오차역전파법을 사용한 학습 구현하기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "652f2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14103333333333334 0.1445\n",
      "0.9058833333333334 0.9088\n",
      "0.9227833333333333 0.9258\n",
      "0.9329166666666666 0.9329\n",
      "0.94555 0.9445\n",
      "0.95175 0.9499\n",
      "0.95735 0.9551\n",
      "0.9621666666666666 0.9596\n",
      "0.9641166666666666 0.9598\n",
      "0.9643 0.9587\n",
      "0.9704833333333334 0.9666\n",
      "0.9729166666666667 0.9662\n",
      "0.9731333333333333 0.9675\n",
      "0.9744166666666667 0.9669\n",
      "0.9764833333333334 0.9702\n",
      "0.9778166666666667 0.9704\n",
      "0.9780333333333333 0.9687\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) #수치미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) #오차역전파법 방식\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
